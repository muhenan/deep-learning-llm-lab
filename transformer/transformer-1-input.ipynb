{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c0548f0-8f7d-4e4f-aba0-f994b1a415f4",
   "metadata": {},
   "source": [
    "# 1. 零件清单 (Core Components)\n",
    "\n",
    "在这一步，我们要实现以下四个核心组件：\n",
    "\n",
    "1. 词表 (Vocabulary)：模拟一个简单的映射。\n",
    "\n",
    "2. 词嵌入 (Embedding)：将数字映射为稠密向量。\n",
    "\n",
    "3. 位置编码 (Positional Encoding)：让模型知道词的顺序。\n",
    "\n",
    "4. 组装核心输入层：将 Embedding 和 Position 相加。\n",
    "\n",
    "我们将使用 PyTorch 的 nn.Module 来构建这些。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242ad5db-8ecf-477d-bef3-0f07cf3cc0b6",
   "metadata": {},
   "source": [
    "## 第一步：环境准备与参数定义\n",
    "\n",
    "首先，我们定义一些基础的“超参数”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "265dfbc6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "# 超参数定义\n",
    "vocab_size = 1000  # 词表大小（假设有1000个常用词）\n",
    "d_model = 512      # 每个单词身份证（向量）的长度\n",
    "max_len = 100      # 句子最大长度\n",
    "n_heads = 3        # 你要求的3个注意力头"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c600182-e923-44d9-89cb-1bad15aa0d4e",
   "metadata": {},
   "source": [
    "## 第二步：位置编码 (Positional Encoding)\n",
    "\n",
    "这是 Transformer 最“数学”的地方。我们使用正弦和余弦函数的组合，这样模型可以通过相对位置来学习顺序。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2810acff-887f-4f12-9638-bac905f0a3b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 创建一个 [max_len, d_model] 的矩阵来存放位置编码\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        \n",
    "        # position 是 [0, 1, 2, ..., max_len-1] 的列向量\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        # 计算分母部分（利用 log 空间计算保证数值稳定性）\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        \n",
    "        # 偶数位置用 sin，奇数位置用 cos\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        # 增加一个 batch 维度变成 [1, max_len, d_model]\n",
    "        pe = pe.unsqueeze(0)\n",
    "        \n",
    "        # register_buffer 的作用是：这个张量是模型的一部分，但不参与梯度更新\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x 的维度是 [batch_size, seq_len, d_model]\n",
    "        # 我们把对应的位置编码加到 x 上\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba6e637-276c-403e-a56c-d67618d66c29",
   "metadata": {},
   "source": [
    "## 第三步：组装零件清单 (Input Layer)\n",
    "\n",
    "现在我们把 Embedding 和 Positional Encoding 封装在一起。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ea515c87-8afd-49a3-a539-d780e0ab9af7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerInput(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, max_len):\n",
    "        super().__init__()\n",
    "        # 1. 词嵌入层：把 ID 变成 512 维向量\n",
    "        self.embedding = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "        # 2. 位置编码层\n",
    "        self.pos_encoding = PositionalEncoding(d_model, max_len)\n",
    "        \n",
    "        # 3. Dropout 层：训练中随机丢弃一部分神经元，防止过拟合\n",
    "        self.dropout = nn.Dropout(p=0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x (词 ID): [batch_size, seq_len]\n",
    "        \n",
    "        # 第一步：ID 变向量\n",
    "        x = self.embedding(x)  # 输出: [batch_size, seq_len, d_model]\n",
    "        \n",
    "        # 第二步：加上位置信息\n",
    "        x = self.pos_encoding(x)\n",
    "        \n",
    "        # 第三步：Dropout\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c42cc800-9d56-4b14-9948-d524f1440652",
   "metadata": {},
   "source": [
    "## 验证一下这部分代码\n",
    "我们可以运行一段测试代码，看看输入 \"I love AI\" 后的变化："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74f50bba-4324-48f2-bb7c-945f8137b11d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "输入 ID 的形状: torch.Size([1, 3])\n",
      "经过核心零件后的形状: torch.Size([1, 3, 512])\n",
      "第一部分的零件已经成功运行！\n"
     ]
    }
   ],
   "source": [
    "# 模拟输入：1行句子，包含3个单词的 ID\n",
    "dummy_input = torch.tensor([[12, 45, 88]]) \n",
    "\n",
    "# 初始化输入层\n",
    "input_layer = TransformerInput(vocab_size, d_model, max_len)\n",
    "\n",
    "# 运行前向传播\n",
    "output = input_layer(dummy_input)\n",
    "\n",
    "print(f\"输入 ID 的形状: {dummy_input.shape}\")      # torch.Size([1, 3])\n",
    "print(f\"经过核心零件后的形状: {output.shape}\")    # torch.Size([1, 3, 512])\n",
    "print(\"第一部分的零件已经成功运行！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11127bac-f333-4ec0-8645-a845c11fdcc3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
